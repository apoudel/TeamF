cost_vector[i] <- sum(cost_per_person$cost)
}
# Explore Outcome ---------------------------------------------------------
results <- data_frame(
p_star = p_star_vector,
cost = cost_vector
) %>%
mutate(avg_cost = cost/nrow(profiles))
# Let's sort in ascending order of cost:
results %>%
arrange(cost) %>%
slice(1:10)
# The optimal p* is here
optima <- results %>%
arrange(cost) %>%
slice(1)
# Define informative plot title
plot_title <- paste("Average Cost per Person where False Negatives Cost ", FN_to_FP_cost_ratio, "x as much as False Positives", sep="")
plot_title
# Plot all the things
ggplot(results, aes(x=p_star, y=avg_cost)) +
geom_point() +
labs(x="p*", y="Avg Cost per Person", title=plot_title) +
geom_vline(xintercept=optima$p_star, linetype="dashed", size=0.5, alpha=0.5) +
geom_hline(yintercept=optima$avg_cost, linetype="dashed", size=0.5, alpha=0.5) +
geom_point(data=optima, col="red", size=5)
View(evals)
library(rpart)
library(tidyverse)
library(broom)
# Load & Prepare Data -----------------------------------------------------
# Based on data from a study conducted at the University of Texas in Austin
# (https://www.openintro.org/stat/data/evals.php) we investigate associations
# between teacher evalutions (scores between 1-5) and attributes of the
# professor including:
# -their rank
# -their ethnicity
# -their gender
# -whether or not english was their mother tongue
# -their age
# -their average "beauty" rating
load(url("http://www.openintro.org/stat/data/evals.RData"))
evals <- evals %>%
tbl_df() %>%
select(score, ethnicity, gender, language, age, bty_avg, rank)
View(evals)
ggplot(evals, aes(x=bty_avg, y=score)) +
geom_jitter() +
geom_hline(yintercept=mean(evals$score), col="red", size=1) +
geom_smooth(method="lm") +
labs(x="Beauty Score", y="Teaching Score")
evals <- evals %>%
mutate(
# Rank: 0 = teaching, 1 = tenure track, 2 = tenure
rank = match(rank, c("teaching", "tenure track", "tenured")) - 1,
# Ethnicity: 0 = non-minority, 1 = minority
ethnicity = match(ethnicity, c("not minority", "minority")) - 1,
# Gender: 0 = male, 1 = female
gender = match(gender, c("male", "female")) - 1,
# Language: 0 = native english speaker, 1 = non-native english speaker
language = match(language, c("english", "non-english")) - 1
)
knobs <- rpart.control(maxdepth = 3)
?rpart.control
model_formula <- as.formula("score ~ ethnicity + gender + language + age + bty_avg + rank")
model_CART <- rpart(model_formula, data = evals, control=knobs)
print(model_CART)
summary(model_CART)
broom::tidy(model_CART)
broom::augment(model_CART)
broom::glance(model_CART)
library(rpart)
library(tidyverse)
library(broom)
# Load & Prepare Data -----------------------------------------------------
# Based on data from a study conducted at the University of Texas in Austin
# (https://www.openintro.org/stat/data/evals.php) we investigate associations
# between teacher evalutions (scores between 1-5) and attributes of the
# professor including:
# -their rank
# -their ethnicity
# -their gender
# -whether or not english was their mother tongue
# -their age
# -their average "beauty" rating
load(url("http://www.openintro.org/stat/data/evals.RData"))
evals <- evals %>%
tbl_df() %>%
select(score, ethnicity, gender, language, age, bty_avg, rank)
View(evals)
# Beauty average? For real? Apparently so:
# 1. The red line is the mean teaching score by itself (using no other information
# about the teachers)
# 2. The blue is the the mean teaching score controlling for beauty. There is
# a significant non-zero (in fact positive) slope.
# i.e. holding all other variables constant, there appears to be a positive
# correlation betwewn beauty score and teaching score. Is it causal? We can't
# say with the info we have.
ggplot(evals, aes(x=bty_avg, y=score)) +
geom_jitter() +
geom_hline(yintercept=mean(evals$score), col="red", size=1) +
geom_smooth(method="lm") +
labs(x="Beauty Score", y="Teaching Score")
# We must first convert all our categorical variables to numerical ones. We use
# the match() function to have the specified coding of the levels of the
# categorical variables.
evals <- evals %>%
mutate(
# Rank: 0 = teaching, 1 = tenure track, 2 = tenure
rank = match(rank, c("teaching", "tenure track", "tenured")) - 1,
# Ethnicity: 0 = non-minority, 1 = minority
ethnicity = match(ethnicity, c("not minority", "minority")) - 1,
# Gender: 0 = male, 1 = female
gender = match(gender, c("male", "female")) - 1,
# Language: 0 = native english speaker, 1 = non-native english speaker
language = match(language, c("english", "non-english")) - 1
)
# Fit CART model ----------------------------------------------------------
# The "depth of tree" is among the many knobs we can control.
knobs <- rpart.control(maxdepth = 3)
# Run the following to see other knobs and stopping criteria
?rpart.control
# We fit the CART model:
model_formula <- as.formula("score ~ ethnicity + gender + language + age + bty_avg + rank")
model_CART <- rpart(model_formula, data = evals, control=knobs)
# Let's study the output:
print(model_CART)
summary(model_CART)
# Blech... Hideous. broom package to the...
broom::tidy(model_CART)
broom::augment(model_CART)
broom::glance(model_CART)
plot(model_CART, margin=0.25)
text(model_CART, use.n = TRUE)
box()
evals %>%
filter(bty_avg >= 2.167) %>%
filter(gender >= 0.5) %>%
group_by(old = age >= 34.5) %>%
summarise(count=n(), mean_eval=mean(score))
predict(model_CART)
evals %>%
sample_n(20) %>%
predict(model_CART, newdata=.)
evals %>%
filter(bty_avg >= 2.167) %>%
filter(gender >= 0.5) %>%
group_by(old = age >= 34.5) %>%
summarise(count=n(), mean_eval=mean(score))
knobs_2 <- rpart.control(maxdepth = 5)
model_CART_2 <- rpart(model_formula, data = evals, control=knobs_2)
# Note: This is base R facetting:
par(mfrow=c(1, 2))
plot(model_CART, margin=0.25)
text(model_CART, use.n = TRUE)
box()
plot(model_CART_2, margin=0.25)
text(model_CART_2, use.n = TRUE)
box()
install("USAboundaries")
package("USAboundaries")
install.packages("USAboundaries")
library(tidyverse)
library(USAboundaries)
library(sp)
library(stringr)
library(broom)
# Load Counties Data -----------------------------------------------------------
# Data consists of observations for 3109 counties in 2000 election
# -(longitude, latitude) of county's centroid i.e. middle point
# -area_sqmi: surface area of county in square miles
# -bush/gore: number of votes for George W. Bush/Al Gore. Other candidates dropped for simplicity.
# -n=bush+gore. NOT the # of votes, but # for Bush + # for Gore
# -pop_density is not true (# of people)/area, but (# of Bush/Gore voters)/area.
elections_county <-
"https://raw.githubusercontent.com/rudeboybert/MATH218/gh-pages/assets/kMeans/elections_2000.csv" %>%
read_csv() %>%
mutate(pop_density = n/area_sqmi)
View(elections_county)
counties_map <- USAboundaries::us_counties("2000-11-01")
states_map <- USAboundaries::us_states("2000-11-01")
plot(counties_map, axes=TRUE)
plot(states_map, axes=TRUE)
# Let's plot in ggplot. broom::tidy() to the rescue!
counties_map_tidy <- counties_map %>%
# set id variable to be FIPS codes
tidy(region="fips") %>%
tbl_df() %>%
# Remove Alaska/Hawaii, who have FIPS codes 02 & 15
filter(!str_sub(id, 1, 2) %in% c("02", "15"))
states_map_tidy <- states_map %>%
tidy(region="abbr_name") %>%
tbl_df() %>%
# Remove Alaska/Hawaii
filter(!id %in% c("AK", "HI"))
ggplot(NULL, aes(x=long, y=lat)) +
# The group aesthetic ensures points in the same polygon are kept together:
geom_polygon(data=counties_map_tidy, aes(group=group), fill="white") +
# Trace outlines of counties then states:
geom_path(data=counties_map_tidy, aes(group=group), col="black", size=0.05) +
geom_path(data=states_map_tidy, aes(group=group), col="black", size=0.3) +
# Use correct aspect ratio for maps:
coord_map()
# Plot 2000 Electoral Map -----------------------------------------------------------
# Overall the lower 48 George W. Bush got 49.7% of vote
overall_bush_prop <- sum(elections_county$bush)/sum(elections_county$n)
elections_map <- counties_map_tidy %>%
inner_join(elections_county, by=c("id"="fips"))
ggplot(NULL, aes(x=long, y=lat)) +
# Fill in county polygons color-coded by %'age margin for push over 49.7%
geom_polygon(data=elections_map, aes(group=group, fill=100*(prop_bush-overall_bush_prop))) +
scale_fill_gradient2(name="% Margin", low="blue", high="red", mid="white") +
geom_path(data=counties_map_tidy, aes(group=group), col="black", size=0.05) +
geom_path(data=states_map_tidy, aes(group=group), col="black", size=0.3) +
coord_map() +
labs(title="% Margin of Bush Victory Above/Below 49.7%")
# kMeans ------------------------------------------------------------------
# Prepare input data
input_data <- elections_county %>%
select(lat_centroid, long_centroid)
# Fit kMeans Clustering
k <- 2
results <- kmeans(input_data, k, nstart = 20)
# Look at results
results$cluster %>% table()
centers <- results$centers %>%
tbl_df()
centers
# Add cluster results to main data:
elections_county <- elections_county %>%
mutate(cluster = as.factor(results$cluster))
# Join with map data and plot
cluster_map <- counties_map_tidy %>%
inner_join(elections_county, by=c("id"="fips"))
ggplot(NULL, aes(x=long, y=lat)) +
geom_polygon(data=cluster_map, aes(group=group, fill=cluster)) +
# Trace outlines:
geom_path(data=counties_map_tidy, aes(group=group), col="black", size=0.05) +
geom_path(data=states_map_tidy, aes(group=group), col="black", size=0.3) +
coord_map() +
# This will only work if you included long_centroid and lat_centroid as X's:
geom_point(data=centers, aes(x=long_centroid, y=lat_centroid), size=3)
# Redo the above code but selecting
# -Different combinations of n, lat_centroid, long_centroid, pop_density,
# prop_bush, etc inside input_data
# -Different numbers of clusters k
install.packages("maptools")
library(tidyverse)
library(USAboundaries)
library(sp)
library(stringr)
library(broom)
# Load Counties Data -----------------------------------------------------------
# Data consists of observations for 3109 counties in 2000 election
# -(longitude, latitude) of county's centroid i.e. middle point
# -area_sqmi: surface area of county in square miles
# -bush/gore: number of votes for George W. Bush/Al Gore. Other candidates dropped for simplicity.
# -n=bush+gore. NOT the # of votes, but # for Bush + # for Gore
# -pop_density is not true (# of people)/area, but (# of Bush/Gore voters)/area.
elections_county <-
"https://raw.githubusercontent.com/rudeboybert/MATH218/gh-pages/assets/kMeans/elections_2000.csv" %>%
read_csv() %>%
mutate(pop_density = n/area_sqmi)
View(elections_county)
counties_map <- USAboundaries::us_counties("2000-11-01")
states_map <- USAboundaries::us_states("2000-11-01")
plot(counties_map, axes=TRUE)
plot(states_map, axes=TRUE)
# Let's plot in ggplot. broom::tidy() to the rescue!
counties_map_tidy <- counties_map %>%
# set id variable to be FIPS codes
tidy(region="fips") %>%
tbl_df() %>%
# Remove Alaska/Hawaii, who have FIPS codes 02 & 15
filter(!str_sub(id, 1, 2) %in% c("02", "15"))
states_map_tidy <- states_map %>%
tidy(region="abbr_name") %>%
tbl_df() %>%
# Remove Alaska/Hawaii
filter(!id %in% c("AK", "HI"))
ggplot(NULL, aes(x=long, y=lat)) +
# The group aesthetic ensures points in the same polygon are kept together:
geom_polygon(data=counties_map_tidy, aes(group=group), fill="white") +
# Trace outlines of counties then states:
geom_path(data=counties_map_tidy, aes(group=group), col="black", size=0.05) +
geom_path(data=states_map_tidy, aes(group=group), col="black", size=0.3) +
# Use correct aspect ratio for maps:
coord_map()
# Plot 2000 Electoral Map -----------------------------------------------------------
# Overall the lower 48 George W. Bush got 49.7% of vote
overall_bush_prop <- sum(elections_county$bush)/sum(elections_county$n)
elections_map <- counties_map_tidy %>%
inner_join(elections_county, by=c("id"="fips"))
ggplot(NULL, aes(x=long, y=lat)) +
# Fill in county polygons color-coded by %'age margin for push over 49.7%
geom_polygon(data=elections_map, aes(group=group, fill=100*(prop_bush-overall_bush_prop))) +
scale_fill_gradient2(name="% Margin", low="blue", high="red", mid="white") +
geom_path(data=counties_map_tidy, aes(group=group), col="black", size=0.05) +
geom_path(data=states_map_tidy, aes(group=group), col="black", size=0.3) +
coord_map() +
labs(title="% Margin of Bush Victory Above/Below 49.7%")
# kMeans ------------------------------------------------------------------
# Prepare input data
input_data <- elections_county %>%
select(lat_centroid, long_centroid)
# Fit kMeans Clustering
k <- 2
results <- kmeans(input_data, k, nstart = 20)
# Look at results
results$cluster %>% table()
centers <- results$centers %>%
tbl_df()
centers
# Add cluster results to main data:
elections_county <- elections_county %>%
mutate(cluster = as.factor(results$cluster))
# Join with map data and plot
cluster_map <- counties_map_tidy %>%
inner_join(elections_county, by=c("id"="fips"))
ggplot(NULL, aes(x=long, y=lat)) +
geom_polygon(data=cluster_map, aes(group=group, fill=cluster)) +
# Trace outlines:
geom_path(data=counties_map_tidy, aes(group=group), col="black", size=0.05) +
geom_path(data=states_map_tidy, aes(group=group), col="black", size=0.3) +
coord_map() +
# This will only work if you included long_centroid and lat_centroid as X's:
geom_point(data=centers, aes(x=long_centroid, y=lat_centroid), size=3)
# Redo the above code but selecting
# -Different combinations of n, lat_centroid, long_centroid, pop_density,
# prop_bush, etc inside input_data
# -Different numbers of clusters k
install.packages("rgeos")
library(tidyverse)
library(USAboundaries)
library(sp)
library(stringr)
library(broom)
library(maptools)
library(rgeos)
# Load Counties Data -----------------------------------------------------------
# Data consists of observations for 3109 counties in 2000 election
# -(longitude, latitude) of county's centroid i.e. middle point
# -area_sqmi: surface area of county in square miles
# -bush/gore: number of votes for George W. Bush/Al Gore. Other candidates dropped for simplicity.
# -n=bush+gore. NOT the # of votes, but # for Bush + # for Gore
# -pop_density is not true (# of people)/area, but (# of Bush/Gore voters)/area.
elections_county <-
"https://raw.githubusercontent.com/rudeboybert/MATH218/gh-pages/assets/kMeans/elections_2000.csv" %>%
read_csv() %>%
mutate(pop_density = n/area_sqmi)
View(elections_county)
counties_map <- USAboundaries::us_counties("2000-11-01")
states_map <- USAboundaries::us_states("2000-11-01")
plot(counties_map, axes=TRUE)
plot(states_map, axes=TRUE)
# Let's plot in ggplot. broom::tidy() to the rescue!
counties_map_tidy <- counties_map %>%
# set id variable to be FIPS codes
tidy(region="fips") %>%
tbl_df() %>%
# Remove Alaska/Hawaii, who have FIPS codes 02 & 15
filter(!str_sub(id, 1, 2) %in% c("02", "15"))
states_map_tidy <- states_map %>%
tidy(region="abbr_name") %>%
tbl_df() %>%
# Remove Alaska/Hawaii
filter(!id %in% c("AK", "HI"))
ggplot(NULL, aes(x=long, y=lat)) +
# The group aesthetic ensures points in the same polygon are kept together:
geom_polygon(data=counties_map_tidy, aes(group=group), fill="white") +
# Trace outlines of counties then states:
geom_path(data=counties_map_tidy, aes(group=group), col="black", size=0.05) +
geom_path(data=states_map_tidy, aes(group=group), col="black", size=0.3) +
# Use correct aspect ratio for maps:
coord_map()
# Plot 2000 Electoral Map -----------------------------------------------------------
# Overall the lower 48 George W. Bush got 49.7% of vote
overall_bush_prop <- sum(elections_county$bush)/sum(elections_county$n)
elections_map <- counties_map_tidy %>%
inner_join(elections_county, by=c("id"="fips"))
ggplot(NULL, aes(x=long, y=lat)) +
# Fill in county polygons color-coded by %'age margin for push over 49.7%
geom_polygon(data=elections_map, aes(group=group, fill=100*(prop_bush-overall_bush_prop))) +
scale_fill_gradient2(name="% Margin", low="blue", high="red", mid="white") +
geom_path(data=counties_map_tidy, aes(group=group), col="black", size=0.05) +
geom_path(data=states_map_tidy, aes(group=group), col="black", size=0.3) +
coord_map() +
labs(title="% Margin of Bush Victory Above/Below 49.7%")
# kMeans ------------------------------------------------------------------
# Prepare input data
input_data <- elections_county %>%
select(lat_centroid, long_centroid)
# Fit kMeans Clustering
k <- 2
results <- kmeans(input_data, k, nstart = 20)
# Look at results
results$cluster %>% table()
centers <- results$centers %>%
tbl_df()
centers
# Add cluster results to main data:
elections_county <- elections_county %>%
mutate(cluster = as.factor(results$cluster))
# Join with map data and plot
cluster_map <- counties_map_tidy %>%
inner_join(elections_county, by=c("id"="fips"))
ggplot(NULL, aes(x=long, y=lat)) +
geom_polygon(data=cluster_map, aes(group=group, fill=cluster)) +
# Trace outlines:
geom_path(data=counties_map_tidy, aes(group=group), col="black", size=0.05) +
geom_path(data=states_map_tidy, aes(group=group), col="black", size=0.3) +
coord_map() +
# This will only work if you included long_centroid and lat_centroid as X's:
geom_point(data=centers, aes(x=long_centroid, y=lat_centroid), size=3)
# Redo the above code but selecting
# -Different combinations of n, lat_centroid, long_centroid, pop_density,
# prop_bush, etc inside input_data
# -Different numbers of clusters k
#-------------------------------------------------------------------------------
# Spring 2017 MATH 218 Statistical Learning Final Project
# Due Tuesday 2017/5/23 12:00pm
#
# Team Members: Aayam Poudel and David Valentin
# Kaggle Competition Name: Rossman Store Sales
# Kaggle Competition URL: https://www.kaggle.com/c/rossmann-store-sales
#-------------------------------------------------------------------------------
# 1. Load All Necessary Packages ------------------------------------------
library(tidyverse)
library(broom)
library(lubridate)
#sexy themes
install.packages('ggthemes')
library(ggthemes)
library(glmnet)
library(bisoreg)
# 2. Load Data Files & Data Cleaning --------------------------------------
#Aayam's working directory
setwd("~/Desktop/Junior Spring/Statistical Learning/Homework/Final/Team_F")
#David's working directory
#setwd("/TeamF")
train <- read_csv("./Files/train.csv")
test <- read_csv("./Files/test.csv")
sample_submissions <- read_csv("Files/sample_submission.csv")
#supplemetal information about the stores:
store <- read_csv("./Files/store.csv")
#since the store info is relevant to both train and test datasets let's join them to the datasets
train <- left_join(train,store, by="Store")
test <- left_join(test,store,by="Store")
#Since the dataset is too big, we couldn't do proper conputations so we decided to make a subset of it
train <- train %>%
sample_frac(0.1)
modified_train <- train %>%
mutate(Year=year(Date), Month=month(Date), DayOfMonth=day(Date)) %>%
select(-Date, -Promo2SinceWeek, -PromoInterval, -Promo2SinceYear, -CompetitionOpenSinceMonth, -CompetitionOpenSinceYear) %>%
mutate(StateHoliday = ifelse(is.na(StateHoliday), 0, StateHoliday)) %>%
mutate(CompetitionDistance = ifelse(is.na(CompetitionDistance), 0, CompetitionDistance)) %>%
group_by(Store) %>%
mutate(avg_sales_by_storetype = mean(Sales)) %>%
mutate(avg_customers_by_storetype = mean(Customers)) %>%
mutate(avg_distance_by_storetype = mean(CompetitionDistance)) %>%
mutate(total_num_promotions = sum(Promo2)) %>%
mutate(Sales_Per_Customer = signif(Sales/Customers, 5)) %>%
mutate(Sales_Per_Customer = ifelse(is.na(Sales_Per_Customer), 0, Sales_Per_Customer))
modified_test <- test %>%
mutate(Year=year(Date), Month=month(Date), DayOfMonth=day(Date)) %>%
select(-Date, -Promo2SinceWeek, -PromoInterval, -Promo2SinceYear, -CompetitionOpenSinceMonth, -CompetitionOpenSinceYear) %>%
mutate(CompetitionDistance = ifelse(is.na(CompetitionDistance), 0, CompetitionDistance)) %>%
mutate(StateHoliday = ifelse(is.na(StateHoliday), 0, StateHoliday)) %>%
group_by(Store) %>%
mutate(avg_distance_by_storetype = mean(CompetitionDistance)) %>%
mutate(total_num_promotions = sum(Promo2))
# 3. Top 4-5 Visualizations/Tables of EDA ---------------------------------
# USE DAVID'S STUFF
# 4. Cross-Validation of Final Model --------------------------------------
# 5. Create Submission ----------------------------------------------------
# 6. Extras ---------------------------------------------------------------
# Section A
#splines
# Section B
#loess
# Section C
#ridge
model_formula <- Sales ~ Store + DayOfWeek + Open + StateHoliday + SchoolHoliday + CompetitionDistance + Promo2 + Year + Month + DayOfMonth + avg_distance_by_storetype + total_num_promotions
X <- model.matrix(model_formula, data = modified_train)[, -1]
y <- modified_train$Sales
n_folds <- 5
#Cross validation for ridge regression
#lambda_values <- 10^seq(from = 2, to = 4, length = 2500)
lambda_values <- 10^seq(from = -2, to = 4, length = 1000)
# Using built-in function, we don't need to use our own for-loops. Note
# alpha=0 for ridge regression
cv_ridge <- cv.glmnet(X, y, alpha = 0, lambda=lambda_values, nfolds = n_folds)
lambda_star_ridge <- cv_ridge %>%
glance() %>%
.[["lambda.min"]]
lambda_star_ridge
# Section D
#lasso
model_formula <- Sales ~ Store + DayOfWeek + Open + StateHoliday + SchoolHoliday + CompetitionDistance + Promo2 + Year + Month + DayOfMonth + avg_distance_by_storetype + total_num_promotions
X <- model.matrix(model_formula, data = modified_train)[, -1]
y <- modified_train$Sales
n_folds <- 5
#Cross validation for ridge regression
#lambda_values <- 10^seq(from = 2, to = 4, length = 2500)
lambda_values <- 10^seq(from = -2, to = 4, length = 1000)
# Using built-in function, we don't need to use our own for-loops. Note
# alpha=1 for lasso
cv_lasso <- cv.glmnet(X, y, alpha = 1, lambda=lambda_values, nfolds = n_folds)
lambda_star_lasso <- cv_lasso %>%
glance() %>%
.[["lambda.min"]]
lambda_star_lasso
